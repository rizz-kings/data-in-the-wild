{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (4.12.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: scrapy in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (22.10.0)\n",
      "Requirement already satisfied: cryptography>=3.4.6 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (41.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (23.2.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (1.21.0)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (68.0.0)\n",
      "Requirement already satisfied: packaging in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (23.1)\n",
      "Requirement already satisfied: tldextract in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: lxml>=4.3.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (4.9.2)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from cryptography>=3.4.6->scrapy) (1.15.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.6.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n",
      "Requirement already satisfied: pyasn1-modules in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: constantly>=15.1 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (4.3.0)\n",
      "Requirement already satisfied: idna in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from tldextract->scrapy) (2.31.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from tldextract->scrapy) (3.9.0)\n",
      "Requirement already satisfied: pycparser in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (2023.7.22)\n",
      "Requirement already satisfied: typing-extensions==4.3.0 in /Users/christianrasmussen/opt/miniconda3/envs/data_wrangling/lib/python3.11/site-packages (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install scrapy\n",
    "!pip install typing-extensions==4.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scrapy.selector import Selector\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = open('../data/raw/urls/imdb.txt', 'r')\n",
    "lines = filepath.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in lines:\n",
    "    url = url+'/reviews'\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(url)\n",
    "    # time.sleep(1)\n",
    "    movie_name = driver.title.split(')')[0]\n",
    "    print(movie_name)\n",
    "    if movie_name in os.listdir('../data/raw/imdb_reviews/'):\n",
    "        pass\n",
    "    # time.sleep(1)\n",
    "\n",
    "    body = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "\n",
    "    sel = Selector(text = driver.page_source)\n",
    "    review_counts = sel.css('.lister .header span::text').extract_first().replace(',','').split(' ')[0]\n",
    "    print(review_counts)\n",
    "    more_review_pages = int(int(review_counts)/25)\n",
    "\n",
    "    for i in tqdm(range(more_review_pages)):\n",
    "        try:\n",
    "            # body.send_keys(Keys.PAGE_DOWN)\n",
    "            # time.sleep(1)\n",
    "            # body.send_keys(Keys.PAGE_DOWN)\n",
    "            # time.sleep(1)\n",
    "            # body.send_keys(Keys.PAGE_DOWN)\n",
    "            # time.sleep(1)\n",
    "            # body.send_keys(Keys.PAGE_DOWN)\n",
    "            # time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            time.sleep(3)\n",
    "            css_selector = 'load-more-trigger'\n",
    "            driver.find_element(By.ID, css_selector).click()\n",
    "            # time.sleep(0.5)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # review = sel2.css('.text.show-more__control::text').extract_first().strip()\n",
    "    # review_date = sel2.css('.review-date::text').extract_first().strip()\n",
    "    # author = sel2.css('.display-name-link a::text').extract_first().strip()\n",
    "    # review_title = sel2.css('a.title::text').extract_first().strip()\n",
    "    # review_url = sel2.css('a.title::attr(href)').extract_first().strip()\n",
    "    # helpfulness = sel2.css('.actions.text-muted::text').extract_first().strip()\n",
    "    # print('nRating:',rating)\n",
    "    # print('nreview_title:',review_title)\n",
    "    # print('nAuthor:',author)\n",
    "    # print('nreview_date:',review_date)\n",
    "    # print('nreview:',review)\n",
    "    # print('nhelpfulness:',helpfulness)\n",
    "\n",
    "\n",
    "    rating_list = []\n",
    "    review_date_list = []\n",
    "    review_title_list = []\n",
    "    author_list = []\n",
    "    review_list = []\n",
    "    review_url_list = []\n",
    "    error_url_list = []\n",
    "    error_msg_list = []\n",
    "    reviews = driver.find_elements(By.CSS_SELECTOR, 'div.review-container')\n",
    "\n",
    "    for d in tqdm(reviews):\n",
    "        try:\n",
    "            sel2 = Selector(text = d.get_attribute('innerHTML'))\n",
    "            try:\n",
    "                rating = sel2.css('.rating-other-user-rating span::text').extract_first()\n",
    "            except:\n",
    "                rating = np.NaN\n",
    "            try:\n",
    "                review = sel2.css('.text.show-more__control::text').extract_first()\n",
    "            except:\n",
    "                review = np.NaN\n",
    "            try:\n",
    "                review_date = sel2.css('.review-date::text').extract_first()\n",
    "            except:\n",
    "                review_date = np.NaN    \n",
    "            try:\n",
    "                author = sel2.css('.display-name-link a::text').extract_first()\n",
    "            except:\n",
    "                author = np.NaN    \n",
    "            try:\n",
    "                review_title = sel2.css('a.title::text').extract_first()\n",
    "            except:\n",
    "                review_title = np.NaN\n",
    "            try:\n",
    "                review_url = sel2.css('a.title::attr(href)').extract_first()\n",
    "            except:\n",
    "                review_url = np.NaN\n",
    "            rating_list.append(rating)\n",
    "            review_date_list.append(review_date)\n",
    "            review_title_list.append(review_title)\n",
    "            author_list.append(author)\n",
    "            review_list.append(review)\n",
    "            review_url_list.append(review_url)\n",
    "        except Exception as e:\n",
    "            error_url_list.append(url)\n",
    "            error_msg_list.append(e)\n",
    "    movie = [movie_name for i in range(len(review_date_list))]\n",
    "    review_df = pd.DataFrame({\n",
    "        'Review_Date':review_date_list,\n",
    "        'Author':author_list,\n",
    "        'Rating':rating_list,\n",
    "        'Review_Title':review_title_list,\n",
    "        'Review':review_list,\n",
    "        'Review_Url':review_url,\n",
    "        'Movie' : movie\n",
    "        })\n",
    "    review_df.to_csv(f'../data/raw/imdb_reviews/{movie_name}.csv')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/imdb_reviews/combined.csv').drop(columns=['Unnamed: 0'])\n",
    "df['Movie'] = df.Movie.apply(lambda x: x.split('_')[:-1])\n",
    "df['Movie'] = df.Movie.apply(lambda x: '-'.join(x))\n",
    "def antman(movie):\n",
    "    if movie == 'AntMan':\n",
    "        return 'ant-man'\n",
    "    else:\n",
    "        return movie.lower()\n",
    "df['Movie'] = df.Movie.apply(lambda x: antman(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_json('../data/raw/imdb_reviews/mcu_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'Review_Date' : 'date',\n",
    "    'Author' : 'user',\n",
    "    'Rating' : 'score',\n",
    "    'Movie' : 'movie',\n",
    "    'Review' : 'text',\n",
    "    'Review_Title' : 'title',\n",
    "    'Review_Url' : 'url'\n",
    "})\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def score(num):\n",
    "    if not math.isnan(num):\n",
    "        return int(num*10)\n",
    "    else:\n",
    "        return num\n",
    "df['score'] = df.score.apply(lambda x: score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet('../data/raw/cleaned_sorta.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
