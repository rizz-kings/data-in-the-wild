{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapes \"Rotten Tomatoes\" reviews using Selenium\n",
    "\n",
    "### Notebook info:\n",
    " - Various functions are put together, they are called at the bottom of the notebook\n",
    " - Uses a list of movie titles (We need to create one at some point)\n",
    " - Can customise how many reviews per movie to scrape\n",
    " - Scrapes both \"Critics\" and \"Audience\" reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import EdgeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "\n",
    "# specify Edge driver location for selenium\n",
    "service = webdriver.EdgeService(executable_path='../selenium/msedgedriver.exe')\n",
    "\n",
    "# run in headless mode\n",
    "# options = EdgeOptions().add_argument('--headless')  # comment out this line to disable headless mode\n",
    "\n",
    "# note to self for listing versions\n",
    "# pip list --format=freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "There are some differences with critic and audience reviews, so I made two sections with a lot of copy paste code but accounting for those differences.\n",
    "\n",
    "Was not bothered to make it nicer in the moment lmao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_browser_instance():\n",
    "    '''For instantiating a selenium browser'\n",
    "    Tries to visit the rottentomatoes homepage\n",
    "    Waits max 30 seconds for it to load (waits for cookie popup to load)\n",
    "    If it loads, clicks reject cookies and returns the instance\n",
    "    If it fails to load, aborts and prints an error message'''\n",
    "    \n",
    "    if 'options' in globals():\n",
    "        browser = webdriver.Edge(service=service, options=options)\n",
    "    else:\n",
    "        browser = webdriver.Edge(service=service)\n",
    "\n",
    "    browser.get('https://www.rottentomatoes.com/')\n",
    "    \n",
    "    # wait until cookie popup appears\n",
    "    reject_button = WebDriverWait(browser, 30).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"onetrust-reject-all-handler\"]'))\n",
    "    )\n",
    "\n",
    "    # click reject button\n",
    "    reject_button.click()\n",
    "    \n",
    "    return browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_critic_reviewer_data(browser, reviews):\n",
    "    '''Given a selenium instance of critic reviews on rotten tomatoes and the reviews dict, uses BeautifulSoup to find all:\n",
    "     - reviewer_name\n",
    "     - review_text\n",
    "     - is_rotten\n",
    "    Adds them to the reviews dict and returns it'''\n",
    "\n",
    "    soup = bs(browser.page_source)\n",
    "    row_soup = soup.find_all('div', class_='review-row')\n",
    "\n",
    "    for row in row_soup:\n",
    "        # names\n",
    "        name = row.select_one('a[data-qa*=review-critic-link]').text.strip()\n",
    "        \n",
    "        # review text\n",
    "        text = row.select_one('p[data-qa*=review-quote]').text.strip()\n",
    "\n",
    "        # reviewer rating\n",
    "        state = row.select_one('score-icon-critic-deprecated')['state']\n",
    "        if state == 'fresh':\n",
    "            rating = False\n",
    "        elif state == 'rotten':\n",
    "            rating = True\n",
    "        else:\n",
    "            print('Something went wrong getting rotten state')\n",
    "\n",
    "        # date of review posted\n",
    "        date = row.select_one('span[data-qa*=review-date]').text.strip()\n",
    "\n",
    "        # adding data to dict\n",
    "        reviews['user'].append(name)\n",
    "        reviews['text'].append(text)\n",
    "        reviews['is_rotten'].append(rating)\n",
    "        reviews['date'].append(date)\n",
    "\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic_url(movie_title):\n",
    "    '''Given the string of a movie title, formats to how it appears in\n",
    "    the rotten tomatoes url. The steps taken here are:\n",
    "     - lowercase all letters\n",
    "     - replace spaces with underscores\n",
    "     - replace dashes with underscores\n",
    "     - removes :'''\n",
    "\n",
    "    # handling annoying specific movie titles\n",
    "    if movie_title == 'the-avengers': movie_title = 'marvels_the_avengers'\n",
    "    if movie_title == 'black-panther': movie_title = 'black_panther_2018'\n",
    "    if movie_title == 'black-widow': movie_title = 'black_widow_2021'\n",
    "    if movie_title == 'doctor-strange': movie_title = 'doctor_strange_2016'\n",
    "\n",
    "    movie_title = movie_title.lower()\n",
    "    movie_title = movie_title.replace(' ', '_')\n",
    "    movie_title = movie_title.replace('-', '_')\n",
    "    movie_title = movie_title.replace(':', '')\n",
    "    movie_title = movie_title.replace('.', '')\n",
    "\n",
    "    url = 'https://www.rottentomatoes.com/m/' + movie_title + '/reviews'\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_critic_reviews(browser, movie_title, N):\n",
    "    '''Given a selenium instance, movie title, and number of reviews to scrape:\n",
    "     - Loads the first page for \"Top Critics\"\n",
    "     - Scrapes the 20 reviews in the page (reviewer name, if movie is rotten or a tomato, and text of review)\n",
    "     - Clicks next button to load more reviews\n",
    "     - Continues until N reviews gathered or all reviews gathered\n",
    "     - Puts reviews in pandas DataFrame\n",
    "     - Returns the DataFrame\n",
    "    '''\n",
    "\n",
    "    # getting full rotten tomatoes url for top critic reviews of given movie\n",
    "    url = create_critic_url(movie_title)\n",
    "\n",
    "    # load first page\n",
    "    browser.get(url)\n",
    "\n",
    "    # wait until the text of first review appears\n",
    "    WebDriverWait(browser, 30).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[2]/div[1]/div[2]/p[1]'))\n",
    "    )\n",
    "\n",
    "    reviews = {'movie': [], 'user': [], 'is_rotten': [], 'date': [], 'text': []}\n",
    "\n",
    "    # scrape initial page\n",
    "    reviews = grab_critic_reviewer_data(browser, reviews)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # keep clicking next page and scraping more reviews\n",
    "    # stops when either the desired number of reviews scraped (N) is reached or when no more reviews to scrape exist\n",
    "    done = False\n",
    "    while done == False:\n",
    "        # Check for if N has been reached\n",
    "        if len(reviews) >= N:\n",
    "            done = True\n",
    "            print(f'Found {N} reviews for {movie_title}')\n",
    "            break\n",
    "\n",
    "        # Check if next button is missing\n",
    "        buttons = browser.find_element(By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[1]')\n",
    "        soup = bs(buttons.get_attribute('innerHTML'))\n",
    "        if soup.find('rt-button', class_='next hide'):\n",
    "            print(f'Only found {len(reviews[\"user\"])} reviews for {movie_title}')\n",
    "            break\n",
    "\n",
    "        # Next button must exist, click it\n",
    "        next_button = browser.find_element(By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[1]/rt-button[2]')\n",
    "        next_button.click()\n",
    "\n",
    "        # wait until buttons have finished loading\n",
    "        while(True):\n",
    "            soup = bs(browser.page_source)\n",
    "            prev_button = soup.select_one('rt-button[class*=prev]').prettify()\n",
    "            next_button = soup.select_one('rt-button[class*=next]').prettify()\n",
    "            if 'disabled' not in prev_button and 'disabled' not in next_button:\n",
    "                break\n",
    "\n",
    "        # scrape it\n",
    "        reviews = grab_critic_reviewer_data(browser, reviews)\n",
    "    \n",
    "    # also adding movie titles\n",
    "    reviews['movie'] = [movie_title] * len(reviews['user'])\n",
    "\n",
    "    # saving to temporary file\n",
    "    df = pd.DataFrame(reviews)\n",
    "    df.to_csv(f'../data/raw/reviews/tmp_RT_critics_{movie_title}.csv', index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_audience_reviewer_data(browser, reviews):\n",
    "    '''Given a selenium instance of critic reviews on rotten tomatoes and the reviews dict, uses BeautifulSoup to find all:\n",
    "     - reviewer_name\n",
    "     - review_text\n",
    "     - review_score\n",
    "    Adds them to the reviews dict and returns it'''\n",
    "\n",
    "    soup = bs(browser.page_source)\n",
    "\n",
    "    # get all rows of reviews\n",
    "    row_soup = soup.find_all('div', class_='audience-review-row')\n",
    "    \n",
    "    for row in row_soup:\n",
    "\n",
    "        # reviewer name (different for verified and non verified users)\n",
    "        if row.select_one('span[data-qa*=review-name]'): # verified user\n",
    "            name = row.select_one('span[data-qa*=review-name]').text.strip()\n",
    "        elif row.select_one('a[data-qa*=review-name]'): # non verified user\n",
    "            name = row.select_one('a[data-qa*=review-name]').text.strip()\n",
    "        else: # sometimes reviewer has no name, no clue why but have to skip\n",
    "            name = None\n",
    "\n",
    "        # review text\n",
    "        review_text = row.select_one('p[data-qa*=review-text]').text.strip()\n",
    "        \n",
    "        # review score\n",
    "        full_stars = len(row.select('span[class*=star-display__filled]'))\n",
    "        half_stars = len(row.select('span[class*=star-display__half]'))\n",
    "        score = full_stars + (half_stars/2)\n",
    "\n",
    "        # date of review\n",
    "        date = row.select_one('span[data-qa*=review-duration]').text.strip()\n",
    "\n",
    "        # appending data to reviews dict\n",
    "        reviews['user'].append(name)\n",
    "        reviews['text'].append(review_text)\n",
    "        reviews['score'].append(score)\n",
    "        reviews['date'].append(date)\n",
    "\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audience_url(movie_title):\n",
    "    '''Given the string of a movie title, formats to how it appears in\n",
    "    the rotten tomatoes url. The steps taken here are:\n",
    "     - lowercase all letters\n",
    "     - replace spaces with underscores\n",
    "     - replace dashes with underscores\n",
    "     - removes :'''\n",
    "    \n",
    "    # handling annoying specific movie titles\n",
    "    if movie_title == 'the-avengers': movie_title = 'marvels_the_avengers'\n",
    "    if movie_title == 'black-panther': movie_title = 'black_panther_2018'\n",
    "    if movie_title == 'black-widow': movie_title = 'black_widow_2021'\n",
    "    if movie_title == 'doctor-strange': movie_title = 'doctor_strange_2016'\n",
    "\n",
    "    movie_title = movie_title.lower()\n",
    "    movie_title = movie_title.replace(' ', '_')\n",
    "    movie_title = movie_title.replace('-', '_')\n",
    "    movie_title = movie_title.replace(':', '')\n",
    "    movie_title = movie_title.replace('.', '')\n",
    "\n",
    "    url = 'https://www.rottentomatoes.com/m/' + movie_title + '/reviews?type=user'\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_audience_reviews(browser, movie_title, N):\n",
    "    '''Given a selenium instance, movie title, and number of reviews to scrape:\n",
    "     - Loads the first page for \"Verified Audience\"\n",
    "     - Scrapes the 20 reviews in the page (reviewer name, 1-5 score of movie, and text of review)\n",
    "     - Clicks next button to load more reviews\n",
    "     - Continues until N reviews gathered or all reviews gathered\n",
    "     - Puts reviews in pandas DataFrame\n",
    "     - Returns the DataFrame\n",
    "    '''\n",
    "\n",
    "    # getting full rotten tomatoes url for top critic reviews of given movie\n",
    "    url = create_audience_url(movie_title)\n",
    "\n",
    "    # load first page\n",
    "    browser.get(url)\n",
    "\n",
    "    # wait until the text of first review appears\n",
    "    WebDriverWait(browser, 30).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[2]/div[2]/div[1]/div[2]/drawer-more/p'))\n",
    "    )\n",
    "\n",
    "    reviews = {'movie': [], 'user': [], 'score': [], 'date': [], 'text': []}\n",
    "\n",
    "    # scrape initial page\n",
    "    reviews = grab_audience_reviewer_data(browser, reviews)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    # keep clicking next page and scraping more reviews\n",
    "    # stops when either the desired number of reviews scraped (N) is reached or when no more reviews to scrape exist\n",
    "    done = False\n",
    "    while done == False:\n",
    "        # Check for if N has been reached\n",
    "        if len(reviews['user']) >= N:\n",
    "            done = True\n",
    "            print(f'Found {N} reviews for {movie_title}')\n",
    "            break\n",
    "\n",
    "        # Check if next button is missing\n",
    "        buttons = browser.find_element(By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[1]')\n",
    "        soup = bs(buttons.get_attribute('innerHTML'))\n",
    "        if soup.find('rt-button', class_='next hide'):\n",
    "            print(f'Only found {len(reviews[\"user\"])} reviews for {movie_title}')\n",
    "            break\n",
    "\n",
    "        # Next button must exist, click it\n",
    "        next_button = browser.find_element(By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[1]/rt-button[2]')\n",
    "        next_button.click()\n",
    "\n",
    "        # wait until buttons have finished loading\n",
    "        count = 0\n",
    "        while(True):\n",
    "            soup = bs(browser.page_source)\n",
    "            prev_button = soup.select_one('rt-button[class*=prev]').prettify()\n",
    "            next_button = soup.select_one('rt-button[class*=next]').prettify()\n",
    "            if 'disabled' not in prev_button and 'disabled' not in next_button:\n",
    "                break\n",
    "            count += 1\n",
    "            if count >= 1000:\n",
    "                print(f'prev/next buttons never left disabled state for some reason... ({movie_title})')\n",
    "                cause_error\n",
    "\n",
    "        # scrape it\n",
    "        reviews = grab_audience_reviewer_data(browser, reviews)\n",
    "    \n",
    "    # also adding movie titles\n",
    "    reviews['movie'] = [movie_title] * len(reviews['user'])\n",
    "    \n",
    "    df = pd.DataFrame(reviews)\n",
    "    df = df.head(N) # keep only N reviews\n",
    "    df.to_csv(f'../data/raw/reviews/tmp_RT_audience_{movie_title}.csv', index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Scrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~50 seconds to run scrapes with these params\n",
    "# eventually use list of movies we have agreed on\n",
    "f = open('../data/raw/movie_stats/mcu_list.json')\n",
    "movie_title_list = json.load(f)\n",
    "reviews_per_movie = 10000\n",
    "\n",
    "# don't scrape already scraped movies\n",
    "scraped_files = [f for f in listdir('../data/raw/reviews/') if isfile(join('../data/raw/reviews/', f))]\n",
    "\n",
    "critic_movie_title_list = []\n",
    "audience_movie_title_list = []\n",
    "for movie_title in movie_title_list:\n",
    "    if f'tmp_RT_critics_{movie_title}.csv' not in scraped_files:\n",
    "        critic_movie_title_list.append(movie_title)\n",
    "    if f'tmp_RT_audience_{movie_title}.csv' not in scraped_files:\n",
    "        audience_movie_title_list.append(movie_title)\n",
    "\n",
    "print(f'Critic   scraping: {reviews_per_movie} reviews from {len(critic_movie_title_list)} movies')\n",
    "print(f'Audience scraping: {reviews_per_movie} reviews from {len(audience_movie_title_list)} movies')\n",
    "print(f'(max {(reviews_per_movie*len(critic_movie_title_list))*2} total reviews)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = open_browser_instance()\n",
    "for movie_title in critic_movie_title_list:\n",
    "    scrape_critic_reviews(browser, movie_title, reviews_per_movie)\n",
    "    print(f'Scraping {movie_title} successful!')\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = open_browser_instance()\n",
    "for movie_title in audience_movie_title_list:\n",
    "    scrape_audience_reviews(browser, movie_title, reviews_per_movie)\n",
    "    print(f'Scraping {movie_title} successful!')\n",
    "    time.sleep(120)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling files into two large files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_files = [f for f in listdir('../data/raw/reviews/') if isfile(join('../data/raw/reviews/', f))]\n",
    "audience_dfs = []\n",
    "critic_dfs = []\n",
    "for file in scraped_files:\n",
    "    if 'audience' in file:\n",
    "        audience_dfs.append(pd.read_csv(f'../data/raw/reviews/{file}'))\n",
    "    if 'critic' in file:\n",
    "        critic_dfs.append(pd.read_csv(f'../data/raw/reviews/{file}'))\n",
    "\n",
    "# concatting the files\n",
    "audience_df = pd.concat(audience_dfs, ignore_index=True, sort=False)\n",
    "critic_df = pd.concat(critic_dfs, ignore_index=True, sort=False)\n",
    "\n",
    "# saving them\n",
    "audience_df.to_csv('../data/raw/reviews/RT_audience.csv', index=False)\n",
    "critic_df.to_csv('../data/raw/reviews/RT_critics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor test for if reading them is easy and if it looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/reviews/RT_critics.csv')\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/reviews/RT_audience.csv')\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting files to be as agreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading them in\n",
    "df_critics = pd.read_csv('../data/raw/reviews/RT_critics_old.csv')\n",
    "df_audience = pd.read_csv('../data/raw/reviews/RT_audience_old.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows where text is nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(df):\n",
    "    '''Given df, removes rows where text is NaN.\n",
    "    returns new df'''\n",
    "    df = df[df['text'].notna()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "critics_test = df_critics.copy()\n",
    "audience_test = df_audience.copy()\n",
    "ct_before = critics_test.shape[0]\n",
    "at_before = audience_test.shape[0]\n",
    "critics_test = remove_nan(critics_test)\n",
    "audience_test = remove_nan(audience_test)\n",
    "ct_after = critics_test.shape[0]\n",
    "at_after = audience_test.shape[0]\n",
    "print(f'Removed {ct_before-ct_after} from critics')\n",
    "print(f'Removed {at_before-at_after} from audience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing quotes unnecessary as they are for pandas to read in data correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change date columns to be pandas datetime format\n",
    "\n",
    "Done using pandas to_datetime converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "at = df_audience.copy()\n",
    "at = convert_date(at)\n",
    "at.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding url columns to files\n",
    "\n",
    "Done by using previously made url functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_url(df):\n",
    "    if 'is_rotten' in df.columns:\n",
    "        df['url'] = df['movie'].transform(lambda x: create_critic_url(x))\n",
    "    else: \n",
    "        df['url'] = df['movie'].transform(lambda x: create_audience_url(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "at = df_audience.copy()\n",
    "ct = df_critics.copy()\n",
    "\n",
    "at = convert_url(at)\n",
    "ct = convert_url(ct)\n",
    "\n",
    "print(at['url'][0])\n",
    "print(ct['url'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing audience ratings from 0-5 to 0-10\n",
    "\n",
    "Done by multiplying score by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_score(df):\n",
    "    df['score'] = df['score'].transform(lambda x: x*2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test = df_audience.copy()\n",
    "print(test['score'][:4])\n",
    "test = convert_score(test)\n",
    "print(test['score'][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline conversion function\n",
    "\n",
    "Just runs all the other functions one after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pipeline(df):\n",
    "    df = remove_nan(df)\n",
    "    df = convert_date(df)\n",
    "    df = convert_url(df)\n",
    "    if 'score' in df.columns:\n",
    "        df = convert_score(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing audience\n",
    "at = df_audience.copy()\n",
    "at = convert_pipeline(at)\n",
    "at.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing critics\n",
    "ct = df_critics.copy()\n",
    "ct = convert_pipeline(ct)\n",
    "ct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and saving conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critics = pd.read_csv('../data/raw/reviews/RT_critics_old.csv')\n",
    "\n",
    "df_critics = convert_pipeline(df_critics)\n",
    "df_audience = convert_pipeline(df_audience)\n",
    "\n",
    "df_critics.to_csv('../data/raw/reviews/RT_critics.csv', index=False)\n",
    "df_audience.to_csv('../data/raw/reviews/RT_audience.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audience = pd.read_csv('../data/raw/reviews/RT_audience.csv')\n",
    "\n",
    "unique_scores = sorted(df_audience['score'].unique())\n",
    "print(unique_scores)\n",
    "print(len(unique_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_wrangling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
