{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapes \"Rotten Tomatoes\" reviews using Selenium\n",
    "\n",
    "### Notebook info:\n",
    " - Various functions are put together, they are called at the bottom of the notebook\n",
    " - Uses a list of movie titles (We need to create one at some point)\n",
    " - Can customise how many reviews per movie to scrape\n",
    " - Scrapes both \"Top Critics\" and \"Verified Audience\" reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "\n",
    "# specify Edge driver location for selenium\n",
    "service = webdriver.EdgeService(executable_path='../selenium/msedgedriver.exe')\n",
    "\n",
    "# note to self for listing versions\n",
    "# pip list --format=freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "There are some differences with critic and audience reviews, so I made two sections with a lot of copy paste code but accounting for those differences.\n",
    "\n",
    "Was not bothered to make it nicer in the moment lmao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_browser_instance():\n",
    "    '''For instantiating a selenium browser'\n",
    "    Tries to visit the rottentomatoes homepage\n",
    "    Waits max 30 seconds for it to load (waits for cookie popup to load)\n",
    "    If it loads, clicks reject cookies and returns the instance\n",
    "    If it fails to load, aborts and prints an error message'''\n",
    "    \n",
    "    browser = webdriver.Edge()\n",
    "    browser.get('https://www.rottentomatoes.com/')\n",
    "    \n",
    "    # wait until cookie popup appears\n",
    "    reject_button = WebDriverWait(browser, 30).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"onetrust-reject-all-handler\"]'))\n",
    "    )\n",
    "\n",
    "    # click reject button\n",
    "    reject_button.click()\n",
    "    \n",
    "    return browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Critic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_critic_reviewer_data(browser, reviews):\n",
    "    '''Given a selenium instance of critic reviews on rotten tomatoes and the reviews dict, uses BeautifulSoup to find all:\n",
    "     - reviewer_name\n",
    "     - review_text\n",
    "     - is_rotten\n",
    "    Adds them to the reviews dict and returns it'''\n",
    "\n",
    "    soup = bs(browser.page_source)\n",
    "\n",
    "    # get names\n",
    "    reviewer_name_soup = soup.find_all('a', class_='display-name')\n",
    "    for i in reviewer_name_soup:\n",
    "        name = i.contents[0].strip()\n",
    "        reviews['reviewer_name'].append(name)\n",
    "    \n",
    "    # get review texts\n",
    "    reviewer_text_soup = soup.find_all('p', class_='review-text')\n",
    "    for i in reviewer_text_soup:\n",
    "        review_text = i.contents[0].strip()\n",
    "        reviews['review_text'].append(review_text)\n",
    "\n",
    "    # get reviewer rating\n",
    "    reviewer_rating_soup = soup.find_all('score-icon-critic-deprecated')\n",
    "    for i in reviewer_rating_soup:\n",
    "        # finds unwanted tags (not sure who they belong to or what) so need to exclude those\n",
    "        # the desired reviews have hidden rotten percentages, we only see if they are fesh or rotten\n",
    "        percentage = i['percentage']\n",
    "        if percentage == 'hide':\n",
    "            rating = i['state']\n",
    "            if rating == 'rotten':\n",
    "                reviews['is_rotten'].append(True)\n",
    "            elif rating == 'fresh':\n",
    "                reviews['is_rotten'].append(False)\n",
    "            else:\n",
    "                print('Something went wrong when checking review rating')\n",
    "                break\n",
    "\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic_url(movie_title):\n",
    "    '''Given the string of a movie title, formats to how it appears in\n",
    "    the rotten tomatoes url. The steps taken here are:\n",
    "     - lowercase all letters\n",
    "     - replace spaces with underscores\n",
    "     - replace dashes with underscores\n",
    "     - removes :'''\n",
    "\n",
    "    movie_title = movie_title.lower()\n",
    "    movie_title = movie_title.replace(' ', '_')\n",
    "    movie_title = movie_title.replace('-', '_')\n",
    "    movie_title = movie_title.replace(':', '')\n",
    "\n",
    "    url = 'https://www.rottentomatoes.com/m/' + movie_title + '/reviews?type=top_critics'\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_critic_reviews(browser, movie_title, N):\n",
    "    '''Given a selenium instance, movie title, and number of reviews to scrape:\n",
    "     - Loads the first page for \"Top Critics\"\n",
    "     - Scrapes the 20 reviews in the page (reviewer name, if movie is rotten or a tomato, and text of review)\n",
    "     - Clicks next button to load more reviews\n",
    "     - Continues until N reviews gathered or all reviews gathered\n",
    "     - Puts reviews in pandas DataFrame\n",
    "     - Returns the DataFrame\n",
    "    '''\n",
    "\n",
    "    # getting full rotten tomatoes url for top critic reviews of given movie\n",
    "    url = create_critic_url(movie_title)\n",
    "\n",
    "    # load first page\n",
    "    browser.get(url)\n",
    "\n",
    "    # wait until the text of first review appears\n",
    "    WebDriverWait(browser, 30).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[2]/div[1]/div[2]/p[1]'))\n",
    "    )\n",
    "\n",
    "    reviews = {'movie_title': [], 'reviewer_name': [], 'review_text': [], 'is_rotten': []}\n",
    "\n",
    "    # scrape initial page\n",
    "    reviews = grab_critic_reviewer_data(browser, reviews)\n",
    "\n",
    "    # keep clicking next page and scraping more reviews\n",
    "    # stops when either the desired number of reviews scraped (N) is reached or when no more reviews to scrape exist\n",
    "    done = False\n",
    "    while done == False:\n",
    "        # Check for if N has been reached\n",
    "        if len(reviews) >= N:\n",
    "            done = True\n",
    "            print(f'Found {N} reviews for {movie_title}')\n",
    "            break\n",
    "\n",
    "        # Issue with not being able to smartly check when prev/next buttons are loaded, so using time.sleep\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Check if next button is missing\n",
    "        buttons = browser.find_element(By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[1]')\n",
    "        soup = bs(buttons.get_attribute('innerHTML'))\n",
    "        if soup.find('rt-button', class_='next hide'):\n",
    "            print(f'Only found {len(reviews[\"reviewer_name\"])} reviews for {movie_title}')\n",
    "            break\n",
    "\n",
    "        # Next button must exist, click it\n",
    "        next_button = browser.find_element(By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[1]/rt-button[2]')\n",
    "        next_button.click()\n",
    "\n",
    "        # Issue with not being able to smartly check when next page is loaded, so using time.sleep\n",
    "        time.sleep(1)\n",
    "\n",
    "        # scrape it\n",
    "        reviews = grab_critic_reviewer_data(browser, reviews)\n",
    "    \n",
    "    # also adding movie titles\n",
    "    reviews['movie_title'] = [movie_title] * len(reviews['reviewer_name'])\n",
    "    \n",
    "    # return pd.DataFrame.from_dict(reviews).astype({'movie_title': str, 'reviewer_name': str, 'review_text': str, 'is_rotten': bool})\n",
    "    return pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verified Audience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_audience_reviewer_data(browser, reviews):\n",
    "    '''Given a selenium instance of critic reviews on rotten tomatoes and the reviews dict, uses BeautifulSoup to find all:\n",
    "     - reviewer_name\n",
    "     - review_text\n",
    "     - review_score\n",
    "    Adds them to the reviews dict and returns it'''\n",
    "\n",
    "    soup = bs(browser.page_source)\n",
    "\n",
    "    # get names\n",
    "    reviewer_name_soup = soup.find_all('div', class_='audience-reviews__name-wrap')\n",
    "    for i in reviewer_name_soup:\n",
    "        name = i.contents[1].text.strip()\n",
    "        reviews['reviewer_name'].append(name)\n",
    "\n",
    "    # get review texts\n",
    "    reviewer_text_soup = soup.find_all('p', class_='audience-reviews__review js-review-text')\n",
    "    for i in reviewer_text_soup:\n",
    "        review_text = i.contents[0].strip()\n",
    "        reviews['review_text'].append(review_text)\n",
    "\n",
    "    # get reviewer rating\n",
    "    reviewer_rating_soup = soup.find_all('span', class_='star-display')\n",
    "    for i in reviewer_rating_soup:\n",
    "        # count number of filled stars\n",
    "        count = 0\n",
    "        for tag in i:\n",
    "            class_name = tag['class'][0]\n",
    "            if class_name == 'star-display__filled':\n",
    "                count += 1\n",
    "        reviews['review_score'].append(count)\n",
    "\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audience_url(movie_title):\n",
    "    '''Given the string of a movie title, formats to how it appears in\n",
    "    the rotten tomatoes url. The steps taken here are:\n",
    "     - lowercase all letters\n",
    "     - replace spaces with underscores\n",
    "     - replace dashes with underscores\n",
    "     - removes :'''\n",
    "\n",
    "    movie_title = movie_title.lower()\n",
    "    movie_title = movie_title.replace(' ', '_')\n",
    "    movie_title = movie_title.replace('-', '_')\n",
    "    movie_title = movie_title.replace(':', '')\n",
    "\n",
    "    url = 'https://www.rottentomatoes.com/m/' + movie_title + '/reviews?type=verified_audience&intcmp=rt-scorecard_audience-score-reviews'\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_audience_reviews(browser, movie_title, N):\n",
    "    '''Given a selenium instance, movie title, and number of reviews to scrape:\n",
    "     - Loads the first page for \"Verified Audience\"\n",
    "     - Scrapes the 20 reviews in the page (reviewer name, 1-5 score of movie, and text of review)\n",
    "     - Clicks next button to load more reviews\n",
    "     - Continues until N reviews gathered or all reviews gathered\n",
    "     - Puts reviews in pandas DataFrame\n",
    "     - Returns the DataFrame\n",
    "    '''\n",
    "\n",
    "    # getting full rotten tomatoes url for top critic reviews of given movie\n",
    "    url = create_audience_url(movie_title)\n",
    "\n",
    "    # load first page\n",
    "    browser.get(url)\n",
    "\n",
    "    # wait until the text of first review appears\n",
    "    \n",
    "    WebDriverWait(browser, 30).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[2]/div[2]/div[1]/div[2]/drawer-more/p'))\n",
    "    )\n",
    "\n",
    "    reviews = {'movie_title': [], 'reviewer_name': [], 'review_text': [], 'review_score': []}\n",
    "\n",
    "    # scrape initial page\n",
    "    reviews = grab_audience_reviewer_data(browser, reviews)\n",
    "\n",
    "    # keep clicking next page and scraping more reviews\n",
    "    # stops when either the desired number of reviews scraped (N) is reached or when no more reviews to scrape exist\n",
    "    done = False\n",
    "    while done == False:\n",
    "        # Check for if N has been reached\n",
    "        if len(reviews['reviewer_name']) >= N:\n",
    "            done = True\n",
    "            print(f'Found {N} reviews for {movie_title}')\n",
    "            break\n",
    "\n",
    "        # Issue with not being able to smartly check when prev/next buttons are loaded, so using time.sleep\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Check if next button is missing\n",
    "        buttons = browser.find_element(By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[1]')\n",
    "        soup = bs(buttons.get_attribute('innerHTML'))\n",
    "        if soup.find('rt-button', class_='next hide'):\n",
    "            print(f'Only found {len(reviews[\"reviewer_name\"])} reviews for {movie_title}')\n",
    "            break\n",
    "\n",
    "        # Next button must exist, click it\n",
    "        next_button = browser.find_element(By.XPATH, '/html/body/div[3]/main/div/div/section/div/div[1]/rt-button[2]')\n",
    "        next_button.click()\n",
    "\n",
    "        # Issue with not being able to smartly check when next page is loaded, so using time.sleep\n",
    "        time.sleep(1)\n",
    "\n",
    "        # scrape it\n",
    "        reviews = grab_audience_reviewer_data(browser, reviews)\n",
    "    \n",
    "    # also adding movie titles\n",
    "    reviews['movie_title'] = [movie_title] * len(reviews['reviewer_name'])\n",
    "    \n",
    "    return pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Scrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~50 seconds to run scrapes with these params\n",
    "# eventually use list of movies we have agreed on\n",
    "movie_title_list = ['Cocaine Bear', 'Spider-Man: No Way Home']\n",
    "reviews_per_movie = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Critics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = open_browser_instance()\n",
    "\n",
    "df_list = []\n",
    "for movie_title in movie_title_list:\n",
    "    df_list.append(scrape_critic_reviews(browser, movie_title, reviews_per_movie))\n",
    "browser.close()\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True, sort=False)\n",
    "print(f'Scraped {df.shape[0]} total reviews!')\n",
    "\n",
    "# saving data\n",
    "df.to_csv('../data/raw/reviews/RT_top_critics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verified Audience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = open_browser_instance()\n",
    "\n",
    "df_list = []\n",
    "for movie_title in movie_title_list:\n",
    "    df_list.append(scrape_audience_reviews(browser, movie_title, reviews_per_movie))\n",
    "browser.close()\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True, sort=False)\n",
    "print(f'Scraped {df.shape[0]} total reviews!')\n",
    "\n",
    "# saving data\n",
    "df.to_csv('../data/raw/reviews/RT_verified_audience.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor test for if reading them is easy and if it looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/reviews/RT_top_critics.csv')\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/reviews/RT_verified_audience.csv')\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_wrangling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
